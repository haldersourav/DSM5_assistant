{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining ideal chunk size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    Settings\n",
    ")\n",
    "from llama_index.core.evaluation import (\n",
    "    DatasetGenerator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "import openai\n",
    "import time\n",
    "import getpass\n",
    "\n",
    "openai_key = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "# os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
    "openai.api_key = openai_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading documents\n",
    "documents = SimpleDirectoryReader(\"./data_samples/\").load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sourav/Projects/genai/lib64/python3.12/site-packages/llama_index/core/evaluation/dataset_generation.py:200: DeprecationWarning: Call to deprecated class DatasetGenerator. (Deprecated in favor of `RagDatasetGenerator` which should be used instead.)\n",
      "  return cls(\n",
      "/home/sourav/Projects/genai/lib64/python3.12/site-packages/llama_index/core/evaluation/dataset_generation.py:296: DeprecationWarning: Call to deprecated class QueryResponseDataset. (Deprecated in favor of `LabelledRagDataset` which should be used instead.)\n",
      "  return QueryResponseDataset(queries=queries, responses=responses_dict)\n"
     ]
    }
   ],
   "source": [
    "# Question generation\n",
    "data_generator = DatasetGenerator.from_documents(documents)\n",
    "eval_questions = data_generator.generate_questions_from_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluators\n",
    "# We will use GPT-4 for evaluating the responses\n",
    "gpt4o = OpenAI(temperature=0, model=\"gpt-4o\")\n",
    "\n",
    "# Define service context for GPT-4 for evaluation\n",
    "# service_context_gpt4o = ServiceContext.from_defaults(llm=gpt4o)\n",
    "Settings.llm = gpt4o\n",
    "\n",
    "# Define Faithfulness and Relevancy Evaluators which are based on GPT-4o\n",
    "faithfulness_gpt4o = FaithfulnessEvaluator()\n",
    "relevancy_gpt4o = RelevancyEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response_time_and_accuracy(chunk_size, eval_questions, eval_documents):\n",
    "    \"\"\"\n",
    "    Evaluate the average response time, faithfulness, and relevancy of responses generated by GPT-3.5-turbo for a given chunk size.\n",
    "    \n",
    "    Parameters:\n",
    "    chunk_size (int): The size of data chunks being processed.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing the average response time, faithfulness, and relevancy metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    total_response_time = 0\n",
    "    total_faithfulness = 0\n",
    "    total_relevancy = 0\n",
    "\n",
    "    # create vector index\n",
    "    # llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "    # service_context = ServiceContext.from_defaults(llm=llm, chunk_size=chunk_size)\n",
    "    Settings.chunk_size = chunk_size\n",
    "    \n",
    "    vector_index = VectorStoreIndex.from_documents(\n",
    "        eval_documents,\n",
    "    )\n",
    "    # build query engine\n",
    "    query_engine = vector_index.as_query_engine()\n",
    "    num_questions = len(eval_questions)\n",
    "\n",
    "    # Iterate over each question in eval_questions to compute metrics.\n",
    "    # While BatchEvalRunner can be used for faster evaluations (see: https://docs.llamaindex.ai/en/latest/examples/evaluation/batch_eval.html),\n",
    "    # we're using a loop here to specifically measure response time for different chunk sizes.\n",
    "    for question in eval_questions:\n",
    "        start_time = time.time()\n",
    "        response_vector = query_engine.query(question)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        faithfulness_result = faithfulness_gpt4o.evaluate_response(\n",
    "            response=response_vector\n",
    "        ).passing\n",
    "        \n",
    "        relevancy_result = relevancy_gpt4o.evaluate_response(\n",
    "            query=question, response=response_vector\n",
    "        ).passing\n",
    "\n",
    "        total_response_time += elapsed_time\n",
    "        total_faithfulness += faithfulness_result\n",
    "        total_relevancy += relevancy_result\n",
    "\n",
    "    average_response_time = total_response_time / num_questions\n",
    "    average_faithfulness = total_faithfulness / num_questions\n",
    "    average_relevancy = total_relevancy / num_questions\n",
    "\n",
    "    return average_response_time, average_faithfulness, average_relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sourav/Projects/genai/lib64/python3.12/site-packages/llama_index/core/evaluation/dataset_generation.py:200: DeprecationWarning: Call to deprecated class DatasetGenerator. (Deprecated in favor of `RagDatasetGenerator` which should be used instead.)\n",
      "  return cls(\n",
      "/home/sourav/Projects/genai/lib64/python3.12/site-packages/llama_index/core/evaluation/dataset_generation.py:296: DeprecationWarning: Call to deprecated class QueryResponseDataset. (Deprecated in favor of `LabelledRagDataset` which should be used instead.)\n",
      "  return QueryResponseDataset(queries=queries, responses=responses_dict)\n"
     ]
    }
   ],
   "source": [
    "eval_documents = documents[:20]\n",
    "data_generator = DatasetGenerator.from_documents(documents[:20])\n",
    "eval_questions = data_generator.generate_questions_from_nodes(num = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What was the goal of enhancing the clinical usefulness of DSM-5?',\n",
       " 'Why are reliable diagnoses essential in the field of mental health?',\n",
       " 'How has the understanding of mental disorders and their treatments evolved over time?',\n",
       " 'What progress has been made in areas such as cognitive neuroscience, brain imaging, epidemiology, and genetics in the last two decades?',\n",
       " 'Why is it important for DSM to evolve in the context of other clinical research initiatives?',\n",
       " 'What arguments support the idea that boundaries between disorder categories are more fluid than previously recognized?',\n",
       " 'How does DSM-5 aim to improve the validity of a diagnosis?',\n",
       " 'Why is clinical training and experience necessary to use DSM for determining a diagnosis?',\n",
       " 'What information is included in the diagnostic criteria of DSM-5?',\n",
       " 'How does DSM-5 aim to provide a clear and concise description of each mental disorder?',\n",
       " 'What was the purpose of the predecessor of DSM published in 1844?',\n",
       " 'How has the understanding of mental disorders evolved according to the DSM-5?',\n",
       " 'Who were the key stakeholders involved in the development and testing of DSM-5?',\n",
       " 'What was the role of patients, families, and advocacy groups in revising DSM-5?',\n",
       " 'How did the DSM evolve after World War II into a diagnostic classification system?',\n",
       " 'What was the goal of DSM-5 in providing guidelines for diagnoses?',\n",
       " 'How was the DSM-5 revision process initiated and coordinated with other organizations?',\n",
       " 'Who were appointed as Chair and Vice-Chair of the DSM-5 Task Force in 2006?',\n",
       " 'What measures were taken to avoid conflicts of interest during the development of DSM-5?',\n",
       " 'How did the disclosure of income and research grants from commercial sources set a new standard for the DSM-5 Task Force members?']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size 128 - Average Response time: 1.74s, Average Faithfulness: 1.00, Average Relevancy: 1.00\n",
      "Chunk size 256 - Average Response time: 2.14s, Average Faithfulness: 1.00, Average Relevancy: 1.00\n",
      "Chunk size 512 - Average Response time: 2.54s, Average Faithfulness: 0.95, Average Relevancy: 0.95\n",
      "Chunk size 1024 - Average Response time: 4.56s, Average Faithfulness: 0.90, Average Relevancy: 0.90\n",
      "Chunk size 2048 - Average Response time: 4.29s, Average Faithfulness: 0.90, Average Relevancy: 0.90\n"
     ]
    }
   ],
   "source": [
    "chunk_sizes = [128, 256, 512, 1024, 2048]\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "  avg_response_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size, eval_questions, eval_documents)\n",
    "  print(f\"Chunk size {chunk_size} - Average Response time: {avg_response_time:.2f}s, Average Faithfulness: {avg_faithfulness:.2f}, Average Relevancy: {avg_relevancy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
